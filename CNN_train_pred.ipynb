{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from unidecode import unidecode_expect_nonascii, unidecode\n",
    "client = MongoClient(connect=False)\n",
    "db = client['newscraper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo service mongod start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_schema(table='articles_cleaned'):\n",
    "    from pprint import pprint\n",
    "    pprint(next(db[table].find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('5a2730f35cedcc6022e9026e'),\n",
      " 'flags': ['left-center', 'very high'],\n",
      " 'source': 'https://brookings.edu',\n",
      " 'text': 'A chronicle of the year that changed Soviet Russia—and molded the '\n",
      "         'future path of one of America’s pre-eminent diplomatic '\n",
      "         'correspondents\\n'\n",
      "         '\\n'\n",
      "         '1956 was an extraordinary year in modern Russian history. It was '\n",
      "         'called “the year of the thaw”—a time when Stalin’s dark legacy of '\n",
      "         'dictatorship died in February only to be reborn later that December. '\n",
      "         'This historic arc from rising hope to crushing despair opened with a '\n",
      "         'speech by Nikita Khrushchev, then the unpredictable leader of the '\n",
      "         'Soviet Union. He astounded everyone by denouncing the one figure '\n",
      "         'who, up to that time, had been hailed as a “genius,” a wizard of '\n",
      "         'communism—Josef Stalin himself. Now, suddenly, this once '\n",
      "         'unassailable god was being portrayed as a “madman” whose '\n",
      "         'idiosyncratic rule had seriously undermined communism and endangered '\n",
      "         'the Soviet state.\\n'\n",
      "         '\\n'\n",
      "         'This amazing switch from hero to villain lifted a heavy overcoat of '\n",
      "         'fear from the backs of ordinary Russians. It also quickly led to '\n",
      "         'anti-communist uprisings in Eastern Europe, none more bloody and '\n",
      "         'challenging than the one in Hungary, which Soviet troops crushed at '\n",
      "         'year’s end.\\n'\n",
      "         '\\n'\n",
      "         'Marvin Kalb, then a young diplomatic attaché at the U.S. Embassy in '\n",
      "         'Moscow, observed this tumultuous year that foretold the end of '\n",
      "         'Soviet communism three decades later. Fluent in Russian, a doctoral '\n",
      "         'candidate at Harvard, he went where few other foreigners would dare '\n",
      "         'go, listening to Russian students secretly attack communism and '\n",
      "         'threaten rebellion against the Soviet system, traveling from one end '\n",
      "         'of a changing country to the other and, thanks to his diplomatic '\n",
      "         'position, meeting and talking with Khrushchev, who playfully '\n",
      "         'nicknamed him Peter the Great.\\n'\n",
      "         '\\n'\n",
      "         'In this, his fifteenth book, Kalb writes a fascinating eyewitness '\n",
      "         'account of a superpower in upheaval and of a people yearning for an '\n",
      "         'end to dictatorship.',\n",
      " 'title': 'The Year I Was Peter the Great',\n",
      " 'url': 'https://www.brookings.edu/book/the-year-i-was-peter-the-great/'}\n"
     ]
    }
   ],
   "source": [
    "show_schema('articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    ''' Retrieves data from MongoDB'''\n",
    "\n",
    "    def __init__(self, db_table='articles', field='text', n_words=20000):\n",
    "\n",
    "        self.n_words = n_words\n",
    "        self.field = field\n",
    "        self.db_table = db_table\n",
    "        self.labels = [\n",
    "            'center', 'conspiracy', 'extreme left', 'extreme right',\n",
    "            'fake news', 'hate', 'high', 'left', 'left-center', 'low', 'mixed',\n",
    "            'pro-science', 'propaganda', 'right', 'right-center', 'satire',\n",
    "            'very high'\n",
    "        ]\n",
    "\n",
    "    def get_all_rows(self):\n",
    "        ''' Retrieve target table from db '''\n",
    "        print(self.n_words)\n",
    "        self.articles = [_ for _ in db[self.db_table].find()\n",
    "                         if _[self.field]]\n",
    "        self.n_articles = len(self.articles)\n",
    "\n",
    "\n",
    "from keras.preprocessing import text as Text\n",
    "\n",
    "\n",
    "class KerasVectorizer(Corpus):\n",
    "    ''' Performs vectorization and text preprocessing '''\n",
    "\n",
    "    def __init__(self, dnn_type='seq', max_len=1000, predict_str=False):\n",
    "        super().__init__()\n",
    "        if not predict_str:\n",
    "            self.get_all_rows()\n",
    "            self.train = True\n",
    "        else:\n",
    "            self.articles = predict_str\n",
    "            self.train = False\n",
    "        self.dnn_type = dnn_type\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def clean(self, seq):\n",
    "        if len(seq):\n",
    "            seq = unidecode(seq)\n",
    "            return ' '.join(\n",
    "                Text.text_to_word_sequence(\n",
    "                    seq,\n",
    "                    filters=\n",
    "                    '''1234567890!\"#$%&()*+,-\\n./—:;<=>?@[\\\\]^_`{|}~\\t\\'“”'''))\n",
    "\n",
    "    def fit(self):\n",
    "        ''' Fit vectorizer on corpus '''\n",
    "\n",
    "        Tokenizer = Text.Tokenizer\n",
    "        tokenizer = Tokenizer(self.n_words)\n",
    "\n",
    "        print('cleaning text')\n",
    "        texts = [self.clean(entry[self.field]) for entry in self.articles]\n",
    "        print('fitting vector')\n",
    "        try:\n",
    "            tokenizer = pickle.load(open('vector234.pkl', 'rb'))\n",
    "        except FileNotFoundError:\n",
    "            tokenizer.fit_on_texts(texts)\n",
    "            pickle.dump(tokenizer, open('vector234.pkl', 'wb'))\n",
    "        self.corpus_vector = tokenizer\n",
    "        self.lookup = {\n",
    "            k: v\n",
    "            for k, v in self.corpus_vector.word_index.items()\n",
    "            if v < self.n_words\n",
    "        }\n",
    "\n",
    "        json.dump(self.lookup, open('lookup234.json', 'w'))\n",
    "\n",
    "    def gen_x_onehot(self):\n",
    "        if self.train:\n",
    "            text = [self.clean(_[self.field]) for _ in self.articles]\n",
    "        else:\n",
    "            text = self.articles\n",
    "        for entry in text:\n",
    "            entry = keras.preprocessing.text.text_to_word_sequence(entry)\n",
    "            yield [self.lookup[word] for word in entry if word in self.lookup]\n",
    "\n",
    "    def transform_x_onehot(self):\n",
    "        x = list(self.gen_x_onehot())\n",
    "        #         v_len = max([len(_)for _ in x])\n",
    "        #         print ('longest text', v_len)\n",
    "        #         if v_len > self.max_len:\n",
    "        #             v_len = self.max_len\n",
    "        self.rev_lookup = {v: k for k, v in self.lookup.items()}\n",
    "        v_len = self.max_len\n",
    "        print('using limit of', v_len)\n",
    "        self.lens = []\n",
    "        for entry in x:\n",
    "            self.lens.append(len(entry))\n",
    "\n",
    "            if len(entry) >= v_len:\n",
    "                yield np.array(entry[-v_len:])\n",
    "            else:\n",
    "                yield np.array([0 for _ in range(v_len - len(entry))] + entry)\n",
    "\n",
    "    def transform_y(self):\n",
    "        ''' Vectorizes y labels '''\n",
    "        for entry in self.articles:\n",
    "            yield np.array(\n",
    "                [1 if _ in entry['flags'] else 0 for _ in self.labels])\n",
    "\n",
    "    def transform_x(self):\n",
    "        ''' Transforms texts to the vector '''\n",
    "\n",
    "        text = [self.clean(_[self.field]) for _ in self.articles]\n",
    "        return self.corpus_vector.texts_to_matrix(text)\n",
    "\n",
    "\n",
    "#         vector = pickle.load(open('./vector234.pkl', 'rb'))\n",
    "\n",
    "#         self.lookup = json.load(open('lookup234.json'))\n",
    "\n",
    "#         return list(self.transform_x_onehot())\n",
    "\n",
    "    def x_y(self):\n",
    "        self.fit()\n",
    "        print('producing x, y data')\n",
    "        y = list(self.transform_y())\n",
    "\n",
    "        if self.dnn_type == 'seq':\n",
    "            x = list(self.transform_x_onehot())\n",
    "        elif self.dnn_type == 'bow':\n",
    "            x = self.transform_x()\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def prep_data():\n",
    "    k_v = KerasVectorizer(max_len=2000)\n",
    "    #http://www.newswhip.com/2013/12/article-length/\n",
    "    x, y = k_v.x_y()\n",
    "    print('data prepared')\n",
    "    print(x[0].shape)\n",
    "\n",
    "    return k_v, x, y\n",
    "\n",
    "\n",
    "def predict_data(text):\n",
    "    k_v = KerasVectorizer(max_len=2000, predict_str=[text])\n",
    "\n",
    "    x = k_v.transform_x()\n",
    "    print('data prepared')\n",
    "    print(x[0].shape)\n",
    "\n",
    "    return k_v, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "cleaning text\n",
      "fitting vector\n",
      "producing x, y data\n",
      "using limit of 2000\n",
      "data prepared\n",
      "(2000,)\n",
      "4458 29724\n",
      "saving pickles\n",
      "[Errno 2] No such file or directory: 'x.pkl'\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "\n",
    "\n",
    "def train_setup():\n",
    "    k_v, X, Y = prep_data()\n",
    "\n",
    "    def val_set(x, y):\n",
    "        val_size = .15\n",
    "        val_ind = int(len(x) * val_size)\n",
    "        print(val_ind, len(x))\n",
    "\n",
    "        randomize = np.arange(len(x))\n",
    "        np.random.shuffle(randomize)\n",
    "\n",
    "        x = np.array(x)[randomize]\n",
    "        y = np.array(y)[randomize]\n",
    "\n",
    "        x = x[:-val_ind]\n",
    "        y = y[:-val_ind]\n",
    "        x_val = x[-val_ind:]\n",
    "        y_val = y[-val_ind:]\n",
    "        assert len(y) == len(x)\n",
    "\n",
    "        return x, y, x_val, y_val\n",
    "\n",
    "    x, y, x_val, y_val = val_set(X, Y)\n",
    "    return x, y, x_val, y_val, k_v\n",
    "\n",
    "\n",
    "def load_pickles():\n",
    "    pickle_rick = 'x', 'y', 'x_val', 'y_val', 'k_v'\n",
    "\n",
    "    for rick in pickle_rick:\n",
    "        yield pickle.load(open(rick + '.pkl', 'rb'))\n",
    "\n",
    "\n",
    "def save_pickles():\n",
    "    x, y, x_val, y_val, k_v = train_setup()\n",
    "    print('saving pickles')\n",
    "    pickle_rick = {'x': x, 'y': y, 'x_val': x_val, 'y_val': y_val, 'k_v': k_v}\n",
    "    for k, v in pickle_rick.items():\n",
    "        yield pickle.dump(v, open(k + '.pkl', 'wb'))\n",
    "\n",
    "\n",
    "try:\n",
    "    x, y, x_val, y_val, k_v = list(load_pickles())\n",
    "    print('pickles loaded')\n",
    "except Exception as e:\n",
    "\n",
    "    #     x, y, x_val, y_val, k_v = train_setup()\n",
    "    \n",
    "    list(save_pickles())\n",
    "    print(e)\n",
    "finally:\n",
    "    x, y, x_val, y_val, k_v = list(load_pickles())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9777\n",
      "[   0    0    0 ... 1641   12 7483]\n",
      "how\n",
      "to\n",
      "spread\n",
      "holiday\n",
      "cheer\n",
      "with\n",
      "your\n",
      "creativity\n",
      "yesterday\n",
      "i\n",
      "took\n",
      "a\n",
      "break\n",
      "from\n",
      "my\n",
      "artwork\n",
      "and\n",
      "writing\n",
      "to\n",
      "enjoy\n",
      "a\n",
      "cup\n",
      "of\n",
      "tea\n",
      "i\n",
      "sat\n",
      "down\n",
      "on\n",
      "the\n",
      "couch\n",
      "my\n",
      "tea\n",
      "and\n",
      "switched\n",
      "on\n",
      "the\n",
      "television\n",
      "news\n",
      "what\n",
      "a\n",
      "mistake\n",
      "that\n",
      "was\n",
      "the\n",
      "news\n",
      "story\n",
      "covered\n",
      "all\n",
      "the\n",
      "black\n",
      "friday\n",
      "insanity\n",
      "going\n",
      "on\n",
      "around\n",
      "the\n",
      "country\n",
      "there\n",
      "were\n",
      "images\n",
      "of\n",
      "people\n",
      "wrestling\n",
      "over\n",
      "merchandise\n",
      "masses\n",
      "of\n",
      "humanity\n",
      "into\n",
      "full\n",
      "of\n",
      "holiday\n",
      "toys\n",
      "clothing\n",
      "and\n",
      "accessories\n",
      "presumably\n",
      "people\n",
      "are\n",
      "vying\n",
      "for\n",
      "the\n",
      "best\n",
      "deals\n",
      "or\n",
      "they\n",
      "want\n",
      "to\n",
      "be\n",
      "the\n",
      "first\n",
      "to\n",
      "popular\n",
      "items\n",
      "before\n",
      "supplies\n",
      "run\n",
      "out\n",
      "yes\n",
      "we\n",
      "are\n",
      "blessed\n",
      "to\n",
      "be\n",
      "free\n",
      "and\n",
      "living\n",
      "in\n",
      "a\n",
      "country\n",
      "with\n",
      "so\n",
      "much\n",
      "abundance\n",
      "but\n",
      "does\n",
      "all\n",
      "this\n",
      "shopping\n",
      "and\n",
      "holiday\n",
      "hustle\n",
      "make\n",
      "us\n",
      "any\n",
      "happier\n",
      "i\n",
      "ll\n",
      "answer\n",
      "my\n",
      "own\n",
      "question\n",
      "and\n",
      "say\n",
      "no\n",
      "we\n",
      "might\n",
      "feel\n",
      "temporarily\n",
      "happy\n",
      "that\n",
      "we\n",
      "got\n",
      "dad\n",
      "a\n",
      "new\n",
      "sweater\n",
      "we\n",
      "might\n",
      "congratulate\n",
      "ourselves\n",
      "for\n",
      "surprising\n",
      "our\n",
      "spouse\n",
      "with\n",
      "that\n",
      "new\n",
      "ipad\n",
      "but\n",
      "deep\n",
      "down\n",
      "the\n",
      "gifts\n",
      "and\n",
      "feelings\n",
      "they\n",
      "produce\n",
      "are\n",
      "fleeting\n",
      "perhaps\n",
      "the\n",
      "answer\n",
      "is\n",
      "to\n",
      "simplify\n",
      "things\n",
      "by\n",
      "tapping\n",
      "our\n",
      "creativity\n",
      "doing\n",
      "so\n",
      "is\n",
      "good\n",
      "for\n",
      "our\n",
      "health\n",
      "and\n",
      "brings\n",
      "joy\n",
      "to\n",
      "others\n",
      "a\n",
      "healthy\n",
      "state\n",
      "of\n",
      "mind\n",
      "an\n",
      "article\n",
      "in\n",
      "co\n",
      "uk\n",
      "noted\n",
      "exercising\n",
      "our\n",
      "creativity\n",
      "can\n",
      "give\n",
      "us\n",
      "an\n",
      "outlet\n",
      "from\n",
      "our\n",
      "daily\n",
      "routines\n",
      "helping\n",
      "us\n",
      "express\n",
      "emotions\n",
      "and\n",
      "boost\n",
      "our\n",
      "happiness\n",
      "levels\n",
      "the\n",
      "article\n",
      "shared\n",
      "the\n",
      "reflections\n",
      "of\n",
      "artist\n",
      "who\n",
      "made\n",
      "four\n",
      "points\n",
      "about\n",
      "how\n",
      "creating\n",
      "art\n",
      "benefits\n",
      "us\n",
      "here\n",
      "are\n",
      "s\n",
      "four\n",
      "points\n",
      "from\n",
      "other\n",
      "worries\n",
      "it\n",
      "is\n",
      "hard\n",
      "to\n",
      "dwell\n",
      "on\n",
      "troubles\n",
      "once\n",
      "in\n",
      "the\n",
      "flow\n",
      "of\n",
      "a\n",
      "painting\n",
      "it\n",
      "has\n",
      "the\n",
      "power\n",
      "to\n",
      "engage\n",
      "you\n",
      "so\n",
      "fully\n",
      "bringing\n",
      "you\n",
      "into\n",
      "the\n",
      "present\n",
      "moment\n",
      "reduces\n",
      "stress\n",
      "studies\n",
      "show\n",
      "that\n",
      "both\n",
      "creating\n",
      "and\n",
      "observing\n",
      "art\n",
      "can\n",
      "reduce\n",
      "the\n",
      "stress\n",
      "hormone\n",
      "doing\n",
      "something\n",
      "you\n",
      "love\n",
      "also\n",
      "releases\n",
      "feel\n",
      "good\n",
      "chemicals\n",
      "that\n",
      "combat\n",
      "stress\n",
      "and\n",
      "reduce\n",
      "pain\n",
      "builds\n",
      "self\n",
      "esteem\n",
      "i\n",
      "feel\n",
      "anything\n",
      "but\n",
      "confident\n",
      "as\n",
      "i\n",
      "start\n",
      "each\n",
      "project\n",
      "painting\n",
      "provides\n",
      "a\n",
      "challenge\n",
      "and\n",
      "with\n",
      "each\n",
      "hour\n",
      "i\n",
      "paint\n",
      "i\n",
      "am\n",
      "building\n",
      "skills\n",
      "it\n",
      "is\n",
      "an\n",
      "activity\n",
      "with\n",
      "a\n",
      "tangible\n",
      "result\n",
      "and\n",
      "the\n",
      "more\n",
      "i\n",
      "dedicate\n",
      "myself\n",
      "slowly\n",
      "but\n",
      "surely\n",
      "the\n",
      "more\n",
      "i\n",
      "can\n",
      "see\n",
      "improvement\n",
      "and\n",
      "feel\n",
      "a\n",
      "sense\n",
      "of\n",
      "achievement\n",
      "creates\n",
      "a\n",
      "healthy\n",
      "state\n",
      "of\n",
      "mind\n",
      "participants\n",
      "in\n",
      "a\n",
      "study\n",
      "who\n",
      "produced\n",
      "art\n",
      "demonstrated\n",
      "a\n",
      "significant\n",
      "improvement\n",
      "in\n",
      "psychological\n",
      "resilience\n",
      "as\n",
      "well\n",
      "as\n",
      "increased\n",
      "levels\n",
      "of\n",
      "functional\n",
      "connectivity\n",
      "in\n",
      "the\n",
      "parts\n",
      "of\n",
      "the\n",
      "brain\n",
      "responsible\n",
      "for\n",
      "self\n",
      "monitoring\n",
      "and\n",
      "memory\n",
      "the\n",
      "study\n",
      "involving\n",
      "participants\n",
      "aged\n",
      "between\n",
      "and\n",
      "also\n",
      "concluded\n",
      "that\n",
      "creating\n",
      "artwork\n",
      "can\n",
      "delay\n",
      "ageing\n",
      "when\n",
      "i\n",
      "was\n",
      "a\n",
      "boy\n",
      "i\n",
      "fell\n",
      "in\n",
      "love\n",
      "with\n",
      "drawing\n",
      "and\n",
      "i\n",
      "spent\n",
      "hours\n",
      "with\n",
      "my\n",
      "drawing\n",
      "pads\n",
      "and\n",
      "it\n",
      "consistently\n",
      "produced\n",
      "a\n",
      "state\n",
      "of\n",
      "mind\n",
      "for\n",
      "me\n",
      "i\n",
      "loved\n",
      "creating\n",
      "christmas\n",
      "cards\n",
      "for\n",
      "family\n",
      "and\n",
      "friends\n",
      "the\n",
      "best\n",
      "part\n",
      "was\n",
      "watching\n",
      "their\n",
      "faces\n",
      "when\n",
      "they\n",
      "opened\n",
      "the\n",
      "cards\n",
      "even\n",
      "in\n",
      "adulthood\n",
      "when\n",
      "my\n",
      "schedule\n",
      "was\n",
      "full\n",
      "i\n",
      "found\n",
      "time\n",
      "to\n",
      "little\n",
      "cartoons\n",
      "inside\n",
      "the\n",
      "christmas\n",
      "cards\n",
      "i\n",
      "mailed\n",
      "out\n",
      "invariably\n",
      "i\n",
      "heard\n",
      "more\n",
      "from\n",
      "people\n",
      "about\n",
      "my\n",
      "cartoons\n",
      "than\n",
      "the\n",
      "bottle\n",
      "of\n",
      "wine\n",
      "or\n",
      "cookies\n",
      "i\n",
      "sent\n",
      "them\n",
      "before\n",
      "i\n",
      "retired\n",
      "last\n",
      "year\n",
      "from\n",
      "the\n",
      "police\n",
      "department\n",
      "i\n",
      "drew\n",
      "a\n",
      "cartoon\n",
      "which\n",
      "served\n",
      "as\n",
      "our\n",
      "department\n",
      "s\n",
      "official\n",
      "holiday\n",
      "card\n",
      "it\n",
      "was\n",
      "much\n",
      "more\n",
      "popular\n",
      "than\n",
      "the\n",
      "commercial\n",
      "cards\n",
      "we\n",
      "typically\n",
      "used\n",
      "speaking\n",
      "of\n",
      "holiday\n",
      "cards\n",
      "i\n",
      "have\n",
      "an\n",
      "artist\n",
      "buddy\n",
      "named\n",
      "seth\n",
      "foley\n",
      "who\n",
      "has\n",
      "crafted\n",
      "personalized\n",
      "greeting\n",
      "cards\n",
      "for\n",
      "years\n",
      "he\n",
      "s\n",
      "both\n",
      "an\n",
      "amazing\n",
      "artist\n",
      "and\n",
      "photo\n",
      "shop\n",
      "me\n",
      "with\n",
      "the\n",
      "artist\n",
      "seth\n",
      "foley\n",
      "r\n",
      "seth\n",
      "s\n",
      "birthday\n",
      "and\n",
      "christmas\n",
      "cards\n",
      "have\n",
      "become\n",
      "something\n",
      "we\n",
      "always\n",
      "look\n",
      "forward\n",
      "to\n",
      "here\n",
      "s\n",
      "an\n",
      "example\n",
      "of\n",
      "seth\n",
      "s\n",
      "it\n",
      "s\n",
      "a\n",
      "image\n",
      "of\n",
      "my\n",
      "wife\n",
      "and\n",
      "i\n",
      "as\n",
      "cast\n",
      "members\n",
      "from\n",
      "the\n",
      "popular\n",
      "tv\n",
      "series\n",
      "i\n",
      "think\n",
      "i\n",
      "looked\n",
      "rather\n",
      "rugged\n",
      "in\n",
      "that\n",
      "surprise\n",
      "them\n",
      "with\n",
      "original\n",
      "art\n",
      "if\n",
      "you\n",
      "like\n",
      "to\n",
      "draw\n",
      "or\n",
      "paint\n",
      "why\n",
      "not\n",
      "craft\n",
      "an\n",
      "picture\n",
      "of\n",
      "your\n",
      "loved\n",
      "one\n",
      "or\n",
      "friend\n",
      "you\n",
      "don\n",
      "t\n",
      "have\n",
      "to\n",
      "be\n",
      "to\n",
      "create\n",
      "something\n",
      "they\n",
      "will\n",
      "treasure\n",
      "in\n",
      "fact\n",
      "your\n",
      "gift\n",
      "will\n",
      "likely\n",
      "be\n",
      "remembered\n",
      "and\n",
      "cherished\n",
      "much\n",
      "longer\n",
      "than\n",
      "any\n",
      "store\n",
      "bought\n",
      "item\n",
      "here\n",
      "s\n",
      "a\n",
      "painting\n",
      "i\n",
      "did\n",
      "years\n",
      "ago\n",
      "of\n",
      "my\n",
      "wife\n",
      "s\n",
      "niece\n",
      "we\n",
      "presented\n",
      "it\n",
      "to\n",
      "the\n",
      "family\n",
      "during\n",
      "christmas\n",
      "and\n",
      "they\n",
      "were\n",
      "thrilled\n",
      "being\n",
      "an\n",
      "artist\n",
      "i\n",
      "m\n",
      "not\n",
      "happy\n",
      "with\n",
      "the\n",
      "painting\n",
      "i\n",
      "see\n",
      "the\n",
      "flaws\n",
      "in\n",
      "it\n",
      "now\n",
      "but\n",
      "that\n",
      "s\n",
      "not\n",
      "the\n",
      "point\n",
      "the\n",
      "point\n",
      "is\n",
      "the\n",
      "painting\n",
      "created\n",
      "joy\n",
      "joy\n",
      "in\n",
      "me\n",
      "for\n",
      "the\n",
      "satisfaction\n",
      "of\n",
      "completing\n",
      "a\n",
      "nice\n",
      "oil\n",
      "painting\n",
      "and\n",
      "joy\n",
      "in\n",
      "the\n",
      "family\n",
      "who\n",
      "received\n",
      "the\n",
      "painting\n",
      "which\n",
      "still\n",
      "hangs\n",
      "proudly\n",
      "in\n",
      "their\n",
      "home\n",
      "such\n",
      "a\n",
      "gift\n",
      "is\n",
      "far\n",
      "more\n",
      "fulfilling\n",
      "to\n",
      "make\n",
      "and\n",
      "receive\n",
      "than\n",
      "any\n",
      "half\n",
      "off\n",
      "item\n",
      "you\n",
      "fought\n",
      "over\n",
      "in\n",
      "a\n",
      "crowded\n",
      "department\n",
      "store\n",
      "you\n",
      "don\n",
      "t\n",
      "have\n",
      "to\n",
      "be\n",
      "a\n",
      "professional\n",
      "artist\n",
      "in\n",
      "order\n",
      "to\n",
      "tap\n",
      "your\n",
      "creativity\n",
      "and\n",
      "for\n",
      "example\n",
      "when\n",
      "we\n",
      "moved\n",
      "to\n",
      "the\n",
      "dessert\n",
      "in\n",
      "southern\n",
      "nevada\n",
      "my\n",
      "wife\n",
      "s\n",
      "girlfriend\n",
      "gave\n",
      "us\n",
      "this\n",
      "clever\n",
      "gift\n",
      "just\n",
      "a\n",
      "in\n",
      "a\n",
      "pot\n",
      "with\n",
      "a\n",
      "clever\n",
      "home\n",
      "made\n",
      "label\n",
      "we\n",
      "still\n",
      "laugh\n",
      "about\n",
      "it\n",
      "when\n",
      "i\n",
      "retired\n",
      "last\n",
      "year\n",
      "after\n",
      "years\n",
      "in\n",
      "law\n",
      "enforcement\n",
      "my\n",
      "wife\n",
      "used\n",
      "my\n",
      "old\n",
      "service\n",
      "boot\n",
      "to\n",
      "create\n",
      "this\n",
      "beautiful\n",
      "holiday\n",
      "display\n",
      "on\n",
      "our\n",
      "front\n",
      "door\n",
      "it\n",
      "celebrated\n",
      "both\n",
      "the\n",
      "christmas\n",
      "season\n",
      "and\n",
      "the\n",
      "thin\n",
      "blue\n",
      "line\n",
      "of\n",
      "law\n",
      "enforcement\n",
      "flowers\n",
      "are\n",
      "always\n",
      "a\n",
      "great\n",
      "gift\n",
      "idea\n",
      "and\n",
      "they\n",
      "can\n",
      "be\n",
      "paired\n",
      "with\n",
      "candles\n",
      "mason\n",
      "jars\n",
      "filled\n",
      "with\n",
      "etc\n",
      "below\n",
      "is\n",
      "a\n",
      "holiday\n",
      "display\n",
      "my\n",
      "wife\n",
      "crafted\n",
      "this\n",
      "is\n",
      "an\n",
      "and\n",
      "creative\n",
      "gift\n",
      "idea\n",
      "that\n",
      "anyone\n",
      "would\n",
      "appreciate\n",
      "receiving\n",
      "the\n",
      "power\n",
      "of\n",
      "the\n",
      "written\n",
      "word\n",
      "let\n",
      "s\n",
      "say\n",
      "you\n",
      "can\n",
      "t\n",
      "draw\n",
      "at\n",
      "all\n",
      "and\n",
      "are\n",
      "allergic\n",
      "to\n",
      "flowers\n",
      "no\n",
      "problem\n",
      "how\n",
      "about\n",
      "poetry\n",
      "never\n",
      "underestimate\n",
      "the\n",
      "power\n",
      "of\n",
      "the\n",
      "written\n",
      "word\n",
      "i\n",
      "met\n",
      "a\n",
      "guy\n",
      "named\n",
      "kevin\n",
      "a\n",
      "few\n",
      "years\n",
      "back\n",
      "outside\n",
      "a\n",
      "bookstore\n",
      "during\n",
      "the\n",
      "holidays\n",
      "he\n",
      "had\n",
      "an\n",
      "old\n",
      "and\n",
      "was\n",
      "selling\n",
      "personalized\n",
      "poems\n",
      "to\n",
      "the\n",
      "poems\n",
      "were\n",
      "out\n",
      "on\n",
      "delicate\n",
      "onion\n",
      "paper\n",
      "it\n",
      "was\n",
      "really\n",
      "cool\n",
      "kevin\n",
      "street\n",
      "poet\n",
      "what\n",
      "s\n",
      "stopping\n",
      "you\n",
      "from\n",
      "filling\n",
      "a\n",
      "with\n",
      "poems\n",
      "and\n",
      "letters\n",
      "for\n",
      "someone\n",
      "special\n",
      "this\n",
      "holiday\n",
      "season\n",
      "get\n",
      "an\n",
      "old\n",
      "and\n",
      "fill\n",
      "it\n",
      "with\n",
      "poems\n",
      "letters\n",
      "and\n",
      "photos\n",
      "imagine\n",
      "their\n",
      "surprise\n",
      "to\n",
      "receive\n",
      "such\n",
      "an\n",
      "unexpected\n",
      "and\n",
      "thoughtful\n",
      "gift\n",
      "the\n",
      "kind\n",
      "of\n",
      "gift\n",
      "likely\n",
      "to\n",
      "remain\n",
      "in\n",
      "their\n",
      "possession\n",
      "for\n",
      "a\n",
      "long\n",
      "time\n",
      "some\n",
      "people\n",
      "fill\n",
      "an\n",
      "entire\n",
      "journal\n",
      "with\n",
      "quotes\n",
      "poems\n",
      "and\n",
      "as\n",
      "a\n",
      "gift\n",
      "idea\n",
      "photos\n",
      "and\n",
      "other\n",
      "images\n",
      "can\n",
      "be\n",
      "inside\n",
      "this\n",
      "kind\n",
      "of\n",
      "gift\n",
      "becomes\n",
      "a\n",
      "that\n",
      "anyone\n",
      "would\n",
      "love\n",
      "to\n",
      "have\n",
      "support\n",
      "other\n",
      "artists\n",
      "if\n",
      "time\n",
      "is\n",
      "tight\n",
      "and\n",
      "you\n",
      "don\n",
      "t\n",
      "feel\n",
      "that\n",
      "creative\n",
      "how\n",
      "about\n",
      "purchasing\n",
      "artwork\n",
      "as\n",
      "a\n",
      "gift\n",
      "there\n",
      "are\n",
      "so\n",
      "many\n",
      "talented\n",
      "artists\n",
      "and\n",
      "out\n",
      "there\n",
      "instead\n",
      "of\n",
      "buying\n",
      "some\n",
      "tired\n",
      "out\n",
      "piece\n",
      "of\n",
      "plastic\n",
      "from\n",
      "china\n",
      "why\n",
      "not\n",
      "invest\n",
      "in\n",
      "original\n",
      "artwork\n",
      "my\n",
      "wife\n",
      "surprised\n",
      "me\n",
      "with\n",
      "this\n",
      "wonderful\n",
      "drawing\n",
      "one\n",
      "year\n",
      "she\n",
      "found\n",
      "an\n",
      "amazing\n",
      "pencil\n",
      "artist\n",
      "named\n",
      "charles\n",
      "randolph\n",
      "bruce\n",
      "who\n",
      "does\n",
      "medieval\n",
      "portraits\n",
      "here\n",
      "is\n",
      "the\n",
      "drawing\n",
      "she\n",
      "gave\n",
      "me\n",
      "pencil\n",
      "drawing\n",
      "by\n",
      "charles\n",
      "randolph\n",
      "bruce\n",
      "whether\n",
      "you\n",
      "craft\n",
      "your\n",
      "own\n",
      "gifts\n",
      "for\n",
      "family\n",
      "and\n",
      "friends\n",
      "or\n",
      "tap\n",
      "the\n",
      "creativity\n",
      "of\n",
      "other\n",
      "artists\n",
      "the\n",
      "outcome\n",
      "will\n",
      "be\n",
      "more\n",
      "memorable\n",
      "holiday\n",
      "gifts\n",
      "let\n",
      "s\n",
      "boycott\n",
      "the\n",
      "and\n",
      "holiday\n",
      "shopping\n",
      "brew\n",
      "yourself\n",
      "a\n",
      "cup\n",
      "of\n",
      "tea\n",
      "on\n",
      "a\n",
      "christmas\n",
      "cookie\n",
      "and\n",
      "make\n",
      "something\n",
      "special\n",
      "by\n",
      "hand\n",
      "be\n",
      "it\n",
      "a\n",
      "cartoon\n",
      "painting\n",
      "poem\n",
      "or\n",
      "floral\n",
      "arrangement\n",
      "all\n",
      "you\n",
      "have\n",
      "to\n",
      "do\n",
      "is\n",
      "carve\n",
      "out\n",
      "a\n",
      "little\n",
      "time\n",
      "and\n",
      "keep\n",
      "away\n",
      "at\n",
      "it\n",
      "keep\n",
      "away\n",
      "like\n",
      "the\n",
      "sky\n",
      "is\n",
      "the\n",
      "limit\n",
      "when\n",
      "it\n",
      "comes\n",
      "to\n",
      "ideas\n",
      "for\n",
      "the\n",
      "holidays\n",
      "whatever\n",
      "you\n",
      "create\n",
      "it\n",
      "s\n",
      "sure\n",
      "to\n",
      "be\n",
      "good\n",
      "for\n",
      "your\n",
      "soul\n",
      "also\n",
      "whoever\n",
      "receives\n",
      "your\n",
      "creative\n",
      "effort\n",
      "will\n",
      "be\n",
      "moved\n",
      "and\n",
      "thankful\n",
      "before\n",
      "you\n",
      "go\n",
      "i\n",
      "m\n",
      "john\n",
      "p\n",
      "weiss\n",
      "i\n",
      "draw\n",
      "cartoons\n",
      "paint\n",
      "landscapes\n",
      "and\n",
      "write\n",
      "about\n",
      "life\n",
      "get\n",
      "on\n",
      "my\n",
      "free\n",
      "email\n",
      "list\n",
      "here\n",
      "and\n",
      "i\n",
      "ll\n",
      "send\n",
      "you\n",
      "eight\n",
      "pages\n",
      "of\n",
      "cartoons\n",
      "and\n",
      "notes\n",
      "on\n",
      "creativity\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(0, len(x))\n",
    "print(i)\n",
    "print(x[i])\n",
    "#print([k_v.labels[n] for n,v in enumerate(y[i]) if v >0])\n",
    "for word in x[i]:\n",
    "    if word:\n",
    "        print(k_v.rev_lookup[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_52 (InputLayer)           (None, 2000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_60 (Embedding)        (None, 2000, 10)     200000      input_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_52 (Reshape)            (None, 2000, 10, 1)  0           embedding_60[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 1999, 1, 128) 2688        reshape_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 1998, 1, 128) 3968        reshape_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_72 (MaxPooling2D) (None, 1, 1, 128)    0           conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_73 (MaxPooling2D) (None, 1, 1, 128)    0           conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 2, 1, 128)    0           max_pooling2d_72[0][0]           \n",
      "                                                                 max_pooling2d_73[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_32 (Flatten)            (None, 256)          0           concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 256)          0           flatten_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_75 (Dense)                (None, 64)           16448       dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 32)           2080        dense_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_77 (Dense)                (None, 17)           561         dense_76[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 225,745\n",
      "Trainable params: 225,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 25266 samples, validate on 4458 samples\n",
      "Epoch 1/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8554Epoch 00001: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 47s 2ms/step - loss: 0.3924 - acc: 0.8554 - val_loss: 0.3592 - val_acc: 0.8668\n",
      "\n",
      "Epoch 2/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8674Epoch 00002: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.3613 - acc: 0.8674 - val_loss: 0.3576 - val_acc: 0.8668\n",
      "\n",
      "Epoch 3/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8673Epoch 00003: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.3593 - acc: 0.8673 - val_loss: 0.3554 - val_acc: 0.8668\n",
      "\n",
      "Epoch 4/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3520 - acc: 0.8694Epoch 00004: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.3520 - acc: 0.8694 - val_loss: 0.3360 - val_acc: 0.8753\n",
      "\n",
      "Epoch 5/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.8755Epoch 00005: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.3345 - acc: 0.8755 - val_loss: 0.3112 - val_acc: 0.8827\n",
      "\n",
      "Epoch 6/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.8807Epoch 00006: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.3183 - acc: 0.8806 - val_loss: 0.2929 - val_acc: 0.8900\n",
      "\n",
      "Epoch 7/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.3046 - acc: 0.8845Epoch 00007: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.3047 - acc: 0.8845 - val_loss: 0.2772 - val_acc: 0.8945\n",
      "\n",
      "Epoch 8/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2913 - acc: 0.8885Epoch 00008: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2913 - acc: 0.8885 - val_loss: 0.2632 - val_acc: 0.8990\n",
      "\n",
      "Epoch 9/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.8918Epoch 00009: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2809 - acc: 0.8918 - val_loss: 0.2512 - val_acc: 0.9027\n",
      "\n",
      "Epoch 10/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2710 - acc: 0.8952Epoch 00010: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2710 - acc: 0.8952 - val_loss: 0.2413 - val_acc: 0.9079\n",
      "\n",
      "Epoch 11/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2622 - acc: 0.8983Epoch 00011: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2621 - acc: 0.8983 - val_loss: 0.2290 - val_acc: 0.9109\n",
      "\n",
      "Epoch 12/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9015Epoch 00012: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2517 - acc: 0.9015 - val_loss: 0.2183 - val_acc: 0.9153\n",
      "\n",
      "Epoch 13/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9044Epoch 00013: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2435 - acc: 0.9044 - val_loss: 0.2069 - val_acc: 0.9192\n",
      "\n",
      "Epoch 14/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2367 - acc: 0.9067Epoch 00014: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2368 - acc: 0.9067 - val_loss: 0.1988 - val_acc: 0.9210\n",
      "\n",
      "Epoch 15/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2289 - acc: 0.9089Epoch 00015: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2289 - acc: 0.9089 - val_loss: 0.1912 - val_acc: 0.9232\n",
      "\n",
      "Epoch 16/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9120Epoch 00016: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2218 - acc: 0.9120 - val_loss: 0.1833 - val_acc: 0.9270\n",
      "\n",
      "Epoch 17/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.9134Epoch 00017: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2169 - acc: 0.9134 - val_loss: 0.1758 - val_acc: 0.9304\n",
      "\n",
      "Epoch 18/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9154Epoch 00018: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2115 - acc: 0.9154 - val_loss: 0.1731 - val_acc: 0.9317\n",
      "\n",
      "Epoch 19/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2079 - acc: 0.9165Epoch 00019: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2079 - acc: 0.9166 - val_loss: 0.1695 - val_acc: 0.9338\n",
      "\n",
      "Epoch 20/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.2026 - acc: 0.9190Epoch 00020: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.2026 - acc: 0.9190 - val_loss: 0.1629 - val_acc: 0.9358\n",
      "\n",
      "Epoch 21/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9197Epoch 00021: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1985 - acc: 0.9197 - val_loss: 0.1589 - val_acc: 0.9376\n",
      "\n",
      "Epoch 22/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1938 - acc: 0.9215Epoch 00022: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1938 - acc: 0.9215 - val_loss: 0.1560 - val_acc: 0.9376\n",
      "\n",
      "Epoch 23/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1897 - acc: 0.9236Epoch 00023: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1897 - acc: 0.9236 - val_loss: 0.1500 - val_acc: 0.9401\n",
      "\n",
      "Epoch 24/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1863 - acc: 0.9246Epoch 00024: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1863 - acc: 0.9247 - val_loss: 0.1466 - val_acc: 0.9425\n",
      "\n",
      "Epoch 25/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1839 - acc: 0.9257Epoch 00025: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1839 - acc: 0.9257 - val_loss: 0.1451 - val_acc: 0.9441\n",
      "\n",
      "Epoch 26/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9267Epoch 00026: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1803 - acc: 0.9267 - val_loss: 0.1420 - val_acc: 0.9445\n",
      "\n",
      "Epoch 27/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.9280Epoch 00027: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1781 - acc: 0.9280 - val_loss: 0.1399 - val_acc: 0.9456\n",
      "\n",
      "Epoch 28/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1736 - acc: 0.9300Epoch 00028: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1736 - acc: 0.9300 - val_loss: 0.1351 - val_acc: 0.9467\n",
      "\n",
      "Epoch 29/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1700 - acc: 0.9310Epoch 00029: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1700 - acc: 0.9310 - val_loss: 0.1304 - val_acc: 0.9484\n",
      "\n",
      "Epoch 30/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9318Epoch 00030: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1677 - acc: 0.9318 - val_loss: 0.1299 - val_acc: 0.9496\n",
      "\n",
      "Epoch 31/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9328Epoch 00031: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1666 - acc: 0.9328 - val_loss: 0.1252 - val_acc: 0.9511\n",
      "\n",
      "Epoch 32/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1626 - acc: 0.9340Epoch 00032: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1627 - acc: 0.9340 - val_loss: 0.1222 - val_acc: 0.9523\n",
      "\n",
      "Epoch 33/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1604 - acc: 0.9352Epoch 00033: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1604 - acc: 0.9352 - val_loss: 0.1201 - val_acc: 0.9538\n",
      "\n",
      "Epoch 34/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9356Epoch 00034: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1589 - acc: 0.9356 - val_loss: 0.1194 - val_acc: 0.9543\n",
      "\n",
      "Epoch 35/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9369Epoch 00035: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1564 - acc: 0.9369 - val_loss: 0.1164 - val_acc: 0.9550\n",
      "\n",
      "Epoch 36/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9371Epoch 00036: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1558 - acc: 0.9371 - val_loss: 0.1139 - val_acc: 0.9564\n",
      "\n",
      "Epoch 37/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1534 - acc: 0.9381Epoch 00037: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1534 - acc: 0.9381 - val_loss: 0.1126 - val_acc: 0.9573\n",
      "\n",
      "Epoch 38/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9390Epoch 00038: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1507 - acc: 0.9390 - val_loss: 0.1095 - val_acc: 0.9586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9397Epoch 00039: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1484 - acc: 0.9397 - val_loss: 0.1057 - val_acc: 0.9597\n",
      "\n",
      "Epoch 40/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9412Epoch 00040: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1463 - acc: 0.9412 - val_loss: 0.1057 - val_acc: 0.9602\n",
      "\n",
      "Epoch 41/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.9408Epoch 00041: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1464 - acc: 0.9408 - val_loss: 0.1036 - val_acc: 0.9609\n",
      "\n",
      "Epoch 42/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.9423Epoch 00042: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1434 - acc: 0.9423 - val_loss: 0.1019 - val_acc: 0.9622\n",
      "\n",
      "Epoch 43/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9431Epoch 00043: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1415 - acc: 0.9431 - val_loss: 0.0989 - val_acc: 0.9625\n",
      "\n",
      "Epoch 44/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1409 - acc: 0.9432Epoch 00044: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1410 - acc: 0.9432 - val_loss: 0.0978 - val_acc: 0.9634\n",
      "\n",
      "Epoch 45/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.9440Epoch 00045: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1379 - acc: 0.9440 - val_loss: 0.0978 - val_acc: 0.9635\n",
      "\n",
      "Epoch 46/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9445Epoch 00046: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1374 - acc: 0.9445 - val_loss: 0.0949 - val_acc: 0.9658\n",
      "\n",
      "Epoch 47/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9461Epoch 00047: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1349 - acc: 0.9461 - val_loss: 0.0938 - val_acc: 0.9657\n",
      "\n",
      "Epoch 48/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9463Epoch 00048: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1340 - acc: 0.9463 - val_loss: 0.0916 - val_acc: 0.9667\n",
      "\n",
      "Epoch 49/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9464Epoch 00049: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1334 - acc: 0.9464 - val_loss: 0.0898 - val_acc: 0.9670\n",
      "\n",
      "Epoch 50/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9470Epoch 00050: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1325 - acc: 0.9470 - val_loss: 0.0883 - val_acc: 0.9692\n",
      "\n",
      "Epoch 51/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1301 - acc: 0.9479Epoch 00051: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1301 - acc: 0.9479 - val_loss: 0.0864 - val_acc: 0.9691\n",
      "\n",
      "Epoch 52/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9484Epoch 00052: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1287 - acc: 0.9484 - val_loss: 0.0852 - val_acc: 0.9694\n",
      "\n",
      "Epoch 53/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1278 - acc: 0.9488Epoch 00053: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1277 - acc: 0.9488 - val_loss: 0.0834 - val_acc: 0.9707\n",
      "\n",
      "Epoch 54/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1261 - acc: 0.9496Epoch 00054: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1262 - acc: 0.9496 - val_loss: 0.0818 - val_acc: 0.9713\n",
      "\n",
      "Epoch 55/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1256 - acc: 0.9500Epoch 00055: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1255 - acc: 0.9500 - val_loss: 0.0817 - val_acc: 0.9716\n",
      "\n",
      "Epoch 56/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9507Epoch 00056: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1232 - acc: 0.9507 - val_loss: 0.0805 - val_acc: 0.9714\n",
      "\n",
      "Epoch 57/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9508Epoch 00057: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1232 - acc: 0.9508 - val_loss: 0.0796 - val_acc: 0.9727\n",
      "\n",
      "Epoch 58/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1222 - acc: 0.9515Epoch 00058: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1222 - acc: 0.9515 - val_loss: 0.0776 - val_acc: 0.9730\n",
      "\n",
      "Epoch 59/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9520Epoch 00059: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1205 - acc: 0.9520 - val_loss: 0.0776 - val_acc: 0.9737\n",
      "\n",
      "Epoch 60/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1207 - acc: 0.9523Epoch 00060: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1207 - acc: 0.9523 - val_loss: 0.0758 - val_acc: 0.9747\n",
      "\n",
      "Epoch 61/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1190 - acc: 0.9529Epoch 00061: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1190 - acc: 0.9529 - val_loss: 0.0751 - val_acc: 0.9745\n",
      "\n",
      "Epoch 62/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1181 - acc: 0.9532Epoch 00062: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1181 - acc: 0.9532 - val_loss: 0.0735 - val_acc: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9537Epoch 00063: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1167 - acc: 0.9537 - val_loss: 0.0730 - val_acc: 0.9754\n",
      "\n",
      "Epoch 64/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1147 - acc: 0.9546Epoch 00064: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1147 - acc: 0.9546 - val_loss: 0.0723 - val_acc: 0.9748\n",
      "\n",
      "Epoch 65/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1143 - acc: 0.9548Epoch 00065: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1143 - acc: 0.9548 - val_loss: 0.0693 - val_acc: 0.9764\n",
      "\n",
      "Epoch 66/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9553Epoch 00066: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1132 - acc: 0.9552 - val_loss: 0.0692 - val_acc: 0.9771\n",
      "\n",
      "Epoch 67/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1130 - acc: 0.9551Epoch 00067: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1130 - acc: 0.9551 - val_loss: 0.0684 - val_acc: 0.9777\n",
      "\n",
      "Epoch 68/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1124 - acc: 0.9557Epoch 00068: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1124 - acc: 0.9557 - val_loss: 0.0687 - val_acc: 0.9777\n",
      "\n",
      "Epoch 69/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9564Epoch 00069: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1106 - acc: 0.9564 - val_loss: 0.0671 - val_acc: 0.9772\n",
      "\n",
      "Epoch 70/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1102 - acc: 0.9565Epoch 00070: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1102 - acc: 0.9565 - val_loss: 0.0663 - val_acc: 0.9782\n",
      "\n",
      "Epoch 71/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1101 - acc: 0.9569Epoch 00071: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1101 - acc: 0.9569 - val_loss: 0.0656 - val_acc: 0.9787\n",
      "\n",
      "Epoch 72/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1081 - acc: 0.9575Epoch 00072: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1082 - acc: 0.9575 - val_loss: 0.0649 - val_acc: 0.9785\n",
      "\n",
      "Epoch 73/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9576Epoch 00073: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1079 - acc: 0.9576 - val_loss: 0.0638 - val_acc: 0.9794\n",
      "\n",
      "Epoch 74/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1079 - acc: 0.9576Epoch 00074: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1079 - acc: 0.9576 - val_loss: 0.0635 - val_acc: 0.9794\n",
      "\n",
      "Epoch 75/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9582Epoch 00075: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1064 - acc: 0.9582 - val_loss: 0.0624 - val_acc: 0.9798\n",
      "\n",
      "Epoch 76/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1068 - acc: 0.9581Epoch 00076: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1068 - acc: 0.9581 - val_loss: 0.0615 - val_acc: 0.9799\n",
      "\n",
      "Epoch 77/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1039 - acc: 0.9596Epoch 00077: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1039 - acc: 0.9596 - val_loss: 0.0601 - val_acc: 0.9798\n",
      "\n",
      "Epoch 78/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9595Epoch 00078: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1031 - acc: 0.9595 - val_loss: 0.0604 - val_acc: 0.9797\n",
      "\n",
      "Epoch 79/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9600Epoch 00079: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1025 - acc: 0.9600 - val_loss: 0.0584 - val_acc: 0.9808\n",
      "\n",
      "Epoch 80/80\n",
      "25248/25266 [============================>.]25248/25266 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9593Epoch 00080: saving model to tester.h5\n",
      "25266/25266 [==============================]25266/25266 [==============================] - 46s 2ms/step - loss: 0.1034 - acc: 0.9594 - val_loss: 0.0597 - val_acc: 0.9810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dnn():\n",
    "\n",
    "    Sequential = keras.models.Sequential\n",
    "    load_model = keras.models.load_model\n",
    "    Tokenizer = keras.preprocessing.text.Tokenizer\n",
    "    Activation = keras.layers.Activation\n",
    "    SGD = keras.optimizers.SGD\n",
    "    Adam = keras.optimizers.Adam\n",
    "    BatchNormalization = keras.layers.BatchNormalization\n",
    "    to_categorical = keras.utils.to_categorical\n",
    "    ModelCheckpoint = keras.callbacks.ModelCheckpoint\n",
    "    Embedding = keras.layers.Embedding\n",
    "    Reshape = keras.layers.Reshape\n",
    "    Flatten = keras.layers.Flatten\n",
    "    Dropout = keras.layers.Dropout\n",
    "    Concatenate = keras.layers.Concatenate\n",
    "    Dense = keras.layers.Dense\n",
    "    Model = keras.models.Model\n",
    "    Input = keras.layers.Input\n",
    "    Conv2D = keras.layers.Conv2D\n",
    "    MaxPool2D = keras.layers.MaxPool2D\n",
    "    Conv1D = keras.layers.Conv1D\n",
    "    MaxPool1D = keras.layers.MaxPool1D\n",
    "\n",
    "    n_classes = 17\n",
    "\n",
    "    def define_model_rnn():\n",
    "        vector_len = x[0].shape[0]\n",
    "        vocab_size = k_v.n_words\n",
    "        embedding_dim = 10\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            keras.layers.Embedding(\n",
    "                vocab_size, embedding_dim, input_shape=(vector_len, )))\n",
    "        model.add(keras.layers.GRU(3, dropout=0.2, recurrent_dropout=0.2))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(n_classes, ))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        return model\n",
    "\n",
    "    def define_model():\n",
    "        vector_len = k_v.n_words\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_shape=(vector_len, )))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(n_classes, ))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        return model\n",
    "\n",
    "    def define_model_cnn():\n",
    "\n",
    "        sequence_length = x.shape[1]\n",
    "        vocabulary_size = k_v.n_words\n",
    "        embedding_dim = 10\n",
    "        filter_sizes = [2, 3]\n",
    "        num_filters = 128\n",
    "        drop = 0.5\n",
    "        batch_size = 5\n",
    "\n",
    "        inputs = Input(shape=(sequence_length, ), dtype='int32')\n",
    "        \n",
    "        embedding = Embedding(\n",
    "            input_dim=vocabulary_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=sequence_length)(inputs)\n",
    "        \n",
    "        reshape = Reshape((sequence_length, embedding_dim, 1))(embedding)\n",
    "\n",
    "        conv_0 = Conv2D(\n",
    "            num_filters,\n",
    "            kernel_size=(filter_sizes[0], embedding_dim),\n",
    "            padding='valid',\n",
    "            kernel_initializer='normal',\n",
    "            activation='relu')(reshape)\n",
    "        conv_1 = Conv2D(\n",
    "            num_filters,\n",
    "            kernel_size=(filter_sizes[1], embedding_dim),\n",
    "            padding='valid',\n",
    "            kernel_initializer='normal',\n",
    "            activation='relu')(reshape)\n",
    "        \n",
    "#         conv_2 = Conv2D(\n",
    "#             num_filters,\n",
    "#             kernel_size=(filter_sizes[2], embedding_dim),\n",
    "#             padding='valid',\n",
    "#             kernel_initializer='normal',\n",
    "#             activation='relu')(reshape)\n",
    "\n",
    "        maxpool_0 = MaxPool2D(\n",
    "            pool_size=(sequence_length - filter_sizes[0] + 1, 1),\n",
    "            strides=(1, 1),\n",
    "            padding='valid')(conv_0)\n",
    "        maxpool_1 = MaxPool2D(\n",
    "            pool_size=(sequence_length - filter_sizes[1] + 1, 1),\n",
    "            strides=(1, 1),\n",
    "            padding='valid')(conv_1)\n",
    "        \n",
    "#         maxpool_2 = MaxPool2D(\n",
    "#             pool_size=(sequence_length - filter_sizes[2] + 1, 1),\n",
    "#             strides=(1, 1),\n",
    "#             padding='valid')(conv_2)\n",
    "\n",
    "        concatenated_tensor = Concatenate(axis=1)(\n",
    "            [maxpool_0, maxpool_1])\n",
    "\n",
    "        flatten = Flatten()(concatenated_tensor)\n",
    "        dropout = Dropout(drop)(flatten)\n",
    "        dense1= Dense(64, activation='relu')(dropout)\n",
    "        dense2= Dense(32, activation='relu')(dense1)\n",
    "        output = Dense(units=n_classes, activation='sigmoid')(dense2)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=output)\n",
    "        print (model.summary())\n",
    "        return model\n",
    "\n",
    "\n",
    "    label_dict = {k: i for i, k in enumerate(k_v.labels)}\n",
    "\n",
    "    print('starting training')\n",
    "\n",
    "    def train():\n",
    "\n",
    "        model = define_model_cnn()\n",
    "\n",
    "        embedding_layer_names = set(\n",
    "            layer.name for layer in model.layers\n",
    "            if layer.name.startswith('embedding_')\n",
    "            or layer.name.startswith('dense_'))\n",
    "\n",
    "        #         tb = keras.callbacks.TensorBoard(\n",
    "        #             histogram_freq=0,\n",
    "        #             batch_size=30,\n",
    "        #             log_dir='./logs/test',\n",
    "        #             write_graph=False,\n",
    "        #             write_grads=False,\n",
    "        #             write_images=False,\n",
    "        #             embeddings_freq = 1,\n",
    "        #             embeddings_layer_names=embedding_layer_names,\n",
    "        #             embeddings_metadata='metadata.tsv')\n",
    "\n",
    "        lr1 = Adam(lr=0.00005)\n",
    "        lr2 = Adam(lr=0.0001)\n",
    "        adam = Adam(lr=0.001)\n",
    "        early_stop = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            mode='auto')\n",
    "        checkpointer = ModelCheckpoint(\n",
    "            filepath='tester.h5', verbose=1, save_best_only=False)\n",
    "        \n",
    "        model.compile(\n",
    "            loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "        history = model.fit(\n",
    "            np.array(x),\n",
    "            np.array(y),\n",
    "            epochs=80,\n",
    "            verbose=1,\n",
    "            validation_data=(x_val, y_val),\n",
    "            callbacks=[\n",
    "                #                 tb,\n",
    "                keras.callbacks.TensorBoard(\n",
    "                    log_dir='./logs/CNN', write_graph=False),\n",
    "                early_stop,\n",
    "                checkpointer,\n",
    "            ])\n",
    "\n",
    "    train()\n",
    "\n",
    "\n",
    "dnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "    \n",
    "load_model = keras.models.load_model\n",
    "model = load_model('tester.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label_dict = {i: k for i, k in enumerate(k_v.labels)}\n",
    "\n",
    "preds = [model.predict(np.array(text).reshape(1,-1)) for text in x_val[:10]]\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "preds\n",
    "pred_dict = {\n",
    "    label_dict[i]: round(float(p), 6) for i, p in enumerate([_ for _ in preds[0].flatten()])\n",
    "}\n",
    "\n",
    "\n",
    "final_output = [    {\n",
    "    label_dict[i]: round(float(p), 6) for i, p in enumerate([_ for _ in pred.flatten()])\n",
    "} for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for i in range(15):\n",
    "    t = [\n",
    "        _[0] for _ in sorted(\n",
    "            final_output[i].items(), key=lambda kv: kv[1], reverse=True)[:1]\n",
    "    ]\n",
    "\n",
    "    p = [k_v.labels[j] for j in [k for k, _ in enumerate(y_val[i]) if _ > 0]]\n",
    "    print(t, '\\t', p,len(set(p) & set(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "res = pd.DataFrame(final_output,y_val[:10])\n",
    "res.transpose().plot(kind='barh');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def metadata():\n",
    "    with open('./logs/test/metadata.tsv','w') as meta:\n",
    "        meta.write('word\\tvalue\\n')\n",
    "        \n",
    "        meta.write('NULL\\tNULL\\n')\n",
    "        for k, v in sorted(k_v.lookup.items(),key=lambda kv: kv[1]):\n",
    "            meta.write(k+'\\t'+str(v)+'\\n')\n",
    "        \n",
    "    with open('./logs/test/metadata.tsv') as meta_read:\n",
    "        print(len([_ for _ in meta_read.readlines()]))\n",
    "#         print(meta_read.read()[:100])\n",
    "        return\n",
    "        \n",
    "\n",
    "        \n",
    "def labels():\n",
    "    with open('./logs/test/metadata_labels.tsv','w') as meta:\n",
    "        meta.write('label\\tnumber\\n')\n",
    "        for k, v in enumerate(k_v.labels):\n",
    "            \n",
    "            \n",
    "            meta.write(str(k)+'\\t'+str(v))\n",
    "                \n",
    "            meta.write('\\n')\n",
    "                    \n",
    "            \n",
    "        \n",
    "    with open('./logs/test/metadata_labels.tsv') as meta_read:\n",
    "        print(meta_read.read())\n",
    "#         print(len([_ for _ in meta_read.readlines()]))\n",
    "        \n",
    "        return\n",
    "metadata()\n",
    "# labels()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize():\n",
    "    rank = (_ for _ in k_v.lookup.items())\n",
    "    [next(rank) for _ in range(18999)]\n",
    "    pprint([next(rank) for _ in range(1000)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
